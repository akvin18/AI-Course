{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we consider the idea of inverse matrices and describe a common method for their construction.\n",
    "\n",
    "As a motivation for the idea, let's again consider the system of linear equations written in the matrix form.\n",
    "\n",
    "$$\n",
    "AX = B\n",
    "$$\n",
    "\n",
    "Again, $A$ is a matrix of coefficients that are known, $B$ is a vector of known data, and $X$ is a vector that is unknown.  If $A$, $B$, and $X$ were instead only numbers, we would recognize immediately that the way to solve for $X$ is to divide both sides of the equation by $A$, so long as $A\\neq 0$.  The natural question to ask about the system is *Can we define matrix division?*\n",
    "\n",
    "The answer is *Not quite.*  We can make progress though by understanding that in the case that $A$,$B$, and $X$ are numbers, we could also find the solution by multiplying by $1/A$.  This subtle distinction is important because it means that we do not need to define division.  We only need to find the number, that when multiplied by $A$ gives 1.  This number is called the multiplicative inverse of $A$ and is written as $1/A$, so long as $A\\neq 0$.\n",
    "\n",
    "We can extend this idea to the situation where $A$, $B$, and $X$ are matrices.  In order to solve the system $AX=B$, we want to multiply by a certain matrix, that when multiplied by $A$ will give the identity matrix $I$.  This matrix is known as the **inverse matrix**, and is given the symbol $A^{-1}$.\n",
    "\n",
    "If $A$ is a square matrix we define $A^{-1}$ (read as \"A inverse\") to be the matrix such that the following are true.\n",
    "\n",
    "$$\n",
    "A^{-1}A = I \\hspace{3cm}AA^{-1} = I\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about inverse matrices:\n",
    "\n",
    "1. The matrix must be square in order for this definition to make sense.  If $A$ is not square, it is impossible for both \n",
    "$A^{-1}A$ and $AA^{-1}$ to be defined.\n",
    "2. Not all matrices have inverses.  Matrices that do have inverses are called **invertible** matrices.  Matrices that do not have inverses are called **non-invertible**, or **singular**, matrices.\n",
    "3. If a matrix is invertible, its inverse is unique.\n",
    "\n",
    "Now *if we know* $A^{-1}$, we can solve the system $AX=B$ by multiplying both sides by $A^{-1}$.\n",
    "\n",
    "$$\n",
    "A^{-1}AX = A^{-1}B\n",
    "$$\n",
    "\n",
    "Then $A^{-1}AX = IX = X$, so the solution to the system is $X=A^{-1}B$.  Unfortunately, it is typically not easy to find $A^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of an inverse matrix\n",
    "\n",
    "We take $C$ as an example matrix, and consider how we might build the inverse.\n",
    "\n",
    "$$\n",
    "C = \\left[ \\begin{array}{rrrr} 1 & 0 & 2 & -1 \\\\ 3 & 1 & -3 & 2 \\\\ 2 & 0 & 4 & 4 \\\\ 2 & 1 & -1 & -1 \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Let's think of the matrix product $CC^{-1}= I$ in terms of the columns of $C^{-1}$.  We put focus on the third column as an example, and label those unknown entries with $y_i$.  The \\* entries are uknown as well, but we will ignore them for the moment.\n",
    "\n",
    "$$\n",
    "CC^{-1}=\n",
    "\\left[ \\begin{array}{rrrr} 1 & 0 & 2 & -1 \\\\ 3 & 1 & -3 & 2 \\\\ 2 & 0 & 4 & 4 \\\\ 2 & 1 & -1 & -1 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{rrrr} * & * & y_1& * \\\\ * & * & y_2 & * \\\\ * & * & y_3 & * \\\\ * & * & y_4 & *  \\end{array}\\right]=\n",
    "\\left[ \\begin{array}{rrrr} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{array}\\right]=\n",
    "I\n",
    "$$\n",
    "\n",
    "Recall now that $C$ multiplied by the third column of $C^{-1}$ produces the third column of $I$.  This gives us a linear system to solve for the $y_i$.\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{rrrr} 1 & 0 & 2 & -1 \\\\ 3 & 1 & -3 & 2 \\\\ 2 & 0 & 4 & 4 \\\\ 2 & 1 & -1 & -1 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r}  y_1 \\\\  y_2  \\\\ y_3 \\\\ y_4  \\end{array}\\right]=\n",
    "\\left[ \\begin{array}{r}  0 \\\\  0  \\\\ 1 \\\\ 0  \\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next write a Python function to compute the inverse of a matrix.  In practice finding the inverse of a matrix is a terribly inefficient way of solving a linear system.  We have to solve $n$ systems to just to find the inverse of an $n \\times n$ matrix, so it appears that it takes $n$ times the amount of work that it would to just solve the system by elimination.  Suppose however that we needed to solve a linear system $AX=B$ for *many different vectors* $B$, but the same coefficient matrix $A$.  In that case it might seem appealing to construct $A^{-1}$.\n",
    "\n",
    "In order to keep the computation somewhat efficient, we want to avoid repeating the row operations as much as possible.  In order to construct $A^{-1}$ we need to solve the system $AX_i=Y_i$, where $Y_i$ is the $i$th column of $I$.  This will produce $X$, which is the $i$th column of $A^{-1}$.  Instead of performing elimination on each augmented matrix $[A|Y_i]$, we can augment $A$ with the entire matrix $I$ and perform the required operations on all $Y_i$ at the same time.  For example, if $A$ is a $4\\times 4$ matrix, we will have the following augmented matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "[A|I] = \\left[ \\begin{array}{rrrr|rrrr} \n",
    "* & * & * & * & 1 & 0 & 0 & 0 \\\\ \n",
    "* & * & * & * & 0 & 1 & 0 & 0 \\\\\n",
    "* & * & * & * & 0 & 0 & 1 & 0 \\\\\n",
    "* & * & * & * & 0 & 0 & 0 & 1 \\\\ \n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If $A$ is invertible, the $\\texttt{RowReduction}$ routine from the previous section should return a matrix of the following form. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "[A|I] = \\left[ \\begin{array}{rrrr|rrrr} \n",
    "* & * & * & * & * & * & * & * \\\\ \n",
    "0 & * & * & * & * & * & * & * \\\\\n",
    "0 & 0 & * & * & * & * & * & * \\\\\n",
    "0 & 0 & 0 & * & * & * & * & * \\\\ \n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can then call the $\\texttt{BackSubstitution}$ function once for each column in the right half of this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a matrix is non-invertible then the process above fails.  We have to realize that within the $\\texttt{BackSubstitution}$ routine we divide by the entries along the main diagonal of the upper triangular matrix.  Recall that these entries are in the very important pivot positions. If there is a zero in at least one pivot position, then the original matrix is non-invertible.\n",
    "\n",
    "\n",
    "Suppose for example that after performing  $\\texttt{RowReduction}$ on the augmented matrix $[A|I]$ in the $\\texttt{Inverse}$ routine, the result is as follows. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "[A|I] = \\left[ \\begin{array}{rrrr|rrrr} \n",
    "* & * & * & * & * & * & * & * \\\\ \n",
    "0 & 0 & * & * & * & * & * & * \\\\\n",
    "0 & 0 & * & * & * & * & * & * \\\\\n",
    "0 & 0 & 0 & * & * & * & * & * \\\\ \n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In this case $ \\texttt {BackSubstitution}$ will fail due to the zero in the pivot position of the second row. Hence, $A^{-1}$ does not exist and we can conclude that $A$ is non-invertible.\n",
    "\n",
    "In general we determine if a given matrix is invertible by carrying out the steps of elimination and examining the entries on the main diagonal of the corresponding upper triangular matrix.  The original matrix is invertible if and only if all of those entries are nonzoro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sided inverses\n",
    "\n",
    "When a matrix is not square it cannot be considered invertible by the definition above.  It can be useful however to consider one-sided inverses.  If $A$ is an $m\\times n$ matrix, we say that a matrix $F$ is a **right inverse** of $A$ if $AF=I_m$.  Similarly, we say that a matrix $G$ is a **left inverse** of $A$ if $GA=I_n$.  Note here that both the right inverse $F$ and the left inverse $G$ are $n\\times m$ matrices, $I_m$ is the $m\\times m$ identity matrix, and $I_n$ is the $n\\times n$ identity matrix.\n",
    "\n",
    "It is interesting to note that if a matrix $A$ has *both* a right inverse $F$ and a left inverse $G$ then they must be equal since\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "F = I_nF = (GA)F = G(AF) = GI_m = G.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This means that unless $A$ is square  (with $m=n$) it cannot have both a left inverse and a right inverse.\n",
    "\n",
    "The one-sided inverses are also related to the solution of the linear system $AX=B$.  If $A$ has a right inverse $F$ then the system must have *at least* one solution, $X = FB$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "AX = A(FB) = (AF)B = IB = B\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We shall see later that the system $AX=B$ has *at most* one solution when $A$ has a left inverse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
