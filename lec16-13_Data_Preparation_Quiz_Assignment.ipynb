{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1e239d",
   "metadata": {},
   "source": [
    "Analytical Quiz: NumPy & Pandas\n",
    "\n",
    "1. You're given a dataset with millions of rows. Why are NumPy arrays or Pandas DataFrames preferred over Python lists and dictionaries?\n",
    "\n",
    "A) They are easier to read and debug\n",
    "\n",
    "B) They consume significantly less memory and provide faster computations\n",
    "\n",
    "C) They automatically remove missing data\n",
    "\n",
    "D) They support more data types\n",
    "\n",
    "2. How would you efficiently compute the mean income by city across a large dataset using Pandas?\n",
    "\n",
    "A) Iterate manually over each row and calculate means\n",
    "\n",
    "B) Group the data by city and use .mean() method\n",
    "\n",
    "C) Convert Pandas DataFrame to a Python list, then calculate the mean\n",
    "\n",
    "D) Use NumPy arrays exclusively for mean calculation\n",
    "\n",
    "3. If you find many NaN values in your dataset, which first step should you take to handle them?\n",
    "\n",
    "A) Immediately drop all NaN values\n",
    "\n",
    "B) Fill NaN values with zero\n",
    "\n",
    "C) Investigate the cause and distribution of NaNs\n",
    "\n",
    "D) Randomly replace NaNs with existing data\n",
    "\n",
    "4. To confirm suspicion of highly correlated columns using Pandas, you would:\n",
    "\n",
    "A) Calculate standard deviation\n",
    "\n",
    "B) Perform a .corr() analysis\n",
    "\n",
    "C) Inspect a histogram\n",
    "\n",
    "D) Count unique values\n",
    "\n",
    "5. Which scenario illustrates NumPy's vectorized operations significantly improving performance?\n",
    "\n",
    "A) Reading data from CSV files\n",
    "\n",
    "B) Summing large arrays element-wise\n",
    "\n",
    "C) Printing data to a console\n",
    "\n",
    "D) Loading data into memory\n",
    "\n",
    "Analytical Quiz: Data Gathering\n",
    "\n",
    "1. How would you collect product data from an online store without an API?\n",
    "\n",
    "A) Request database access from the store\n",
    "\n",
    "B) Scrape the website using tools like BeautifulSoup or Selenium\n",
    "\n",
    "C) Ask the store to manually send product data\n",
    "\n",
    "D) Wait for the store to develop an API\n",
    "\n",
    "2. When scraping websites, important ethical and legal considerations include:\n",
    "\n",
    "A) Ignoring robots.txt for more data\n",
    "\n",
    "B) Ensuring you follow terms of service and respecting privacy\n",
    "\n",
    "C) Scraping only at peak traffic times\n",
    "\n",
    "D) Collecting personal data without consent\n",
    "\n",
    "3. To efficiently gather data from an API limited to 1000 records per request:\n",
    "\n",
    "A) Request all data at once\n",
    "\n",
    "B) Batch requests and store data incrementally\n",
    "\n",
    "C) Request fewer than 1000 records each time\n",
    "\n",
    "D) Continuously repeat the same request\n",
    "\n",
    "4. How could you handle failed page loads when scraping a public news site?\n",
    "\n",
    "A) Stop scraping immediately\n",
    "\n",
    "B) Use retries with delays and error handling\n",
    "\n",
    "C) Switch immediately to a different site\n",
    "\n",
    "D) Ignore failed pages entirely\n",
    "\n",
    "5. To reconcile and deduplicate data from multiple overlapping sources, you'd:\n",
    "\n",
    "A) Randomly remove duplicates\n",
    "\n",
    "B) Always keep the latest entry\n",
    "\n",
    "C) Use consistent identifiers and merge techniques\n",
    "\n",
    "D) Avoid using multiple sources\n",
    "\n",
    "Analytical Quiz: Data Cleaning\n",
    "\n",
    "1. When 30% of values in a key column are missing, your first step is to:\n",
    "\n",
    "A) Immediately fill them with zeros\n",
    "\n",
    "B) Delete the entire column\n",
    "\n",
    "C) Analyze why data is missing and evaluate its impact\n",
    "\n",
    "D) Replace them randomly with non-missing values\n",
    "\n",
    "2. To standardize inconsistent category labels like 'NY', 'New York', 'new york', you would:\n",
    "\n",
    "A) Remove all non-standard labels\n",
    "\n",
    "B) Replace them manually each time they appear\n",
    "\n",
    "C) Apply string normalization techniques\n",
    "\n",
    "D) Convert them all to numeric codes\n",
    "\n",
    "3. When outliers skew the mean, an appropriate strategy is to:\n",
    "\n",
    "A) Always remove the outliers\n",
    "\n",
    "B) Investigate outliers and choose between treatment or preservation based on context\n",
    "\n",
    "C) Replace outliers with average values\n",
    "\n",
    "D) Ignore the impact on mean\n",
    "\n",
    "4. To ensure numeric columns are correctly typed, you would:\n",
    "\n",
    "A) Visually inspect data\n",
    "\n",
    "B) Convert columns forcibly and handle errors explicitly\n",
    "\n",
    "C) Trust original data format\n",
    "\n",
    "D) Replace all numeric data with strings\n",
    "\n",
    "5. A repeatable and auditable data cleaning pipeline would ideally:\n",
    "\n",
    "A) Be executed manually each time\n",
    "\n",
    "B) Have clearly documented steps and be automated\n",
    "\n",
    "C) Change methods frequently\n",
    "\n",
    "D) Not log transformations for efficiency\n",
    "\n",
    "Analytical Quiz: Data Augmentation\n",
    "\n",
    "1. For a highly imbalanced dataset (90% no, 10% yes), beneficial augmentation strategies include:\n",
    "\n",
    "A) Randomly deleting the minority class\n",
    "\n",
    "B) Duplicating majority class\n",
    "\n",
    "C) Using synthetic data generation (SMOTE)\n",
    "\n",
    "D) Ignoring imbalance completely\n",
    "\n",
    "2. In text classification with limited data, useful augmentation approaches might be:\n",
    "\n",
    "A) Decreasing vocabulary size\n",
    "\n",
    "B) Introducing random spelling mistakes\n",
    "\n",
    "C) Paraphrasing sentences or using back-translation\n",
    "\n",
    "D) Removing punctuation\n",
    "\n",
    "3. The main difference between feature engineering and data augmentation is:\n",
    "\n",
    "A) Augmentation creates new data points; feature engineering modifies existing features\n",
    "\n",
    "B) Feature engineering always reduces dataset size\n",
    "\n",
    "C) Augmentation directly improves model accuracy\n",
    "\n",
    "D) There is no difference\n",
    "\n",
    "4. Synthetic data generation might introduce bias because:\n",
    "\n",
    "A) It is always perfectly balanced\n",
    "\n",
    "B) It duplicates existing bias in training data\n",
    "\n",
    "C) It only generates neutral examples\n",
    "\n",
    "D) It always reduces variance\n",
    "\n",
    "5. Real-time data augmentation during model training involves:\n",
    "\n",
    "A) Applying augmentations before training starts\n",
    "\n",
    "B) Augmenting data on-the-fly during each training batch\n",
    "\n",
    "C) Using only synthetic data\n",
    "\n",
    "D) Storing augmented data permanently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment: End-to-End Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Build a small pipeline that simulates a real-world data task using NumPy, Pandas, and data preparation techniques.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Generate Data:**  \n",
    "   - Use NumPy to simulate a dataset of 1000 people with fields: `age`, `income`, `city`, and `purchase_status`.\n",
    "   - Add some missing values randomly.\n",
    "\n",
    "2. **Clean Data:**  \n",
    "   - Fill missing ages with the mean.\n",
    "   - Drop rows where `purchase_status` is missing.\n",
    "   - Convert `purchase_status` to binary: yes → 1, no → 0\n",
    "\n",
    "3. **Augment Data:**  \n",
    "   - Add a new feature: `age_group` based on age.\n",
    "   - Perform simple upsampling to balance `purchase_status`.\n",
    "\n",
    "4. **Output:**  \n",
    "   - Save the cleaned and augmented DataFrame as CSV.\n",
    "   - Print basic statistics (mean, value counts, etc.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
