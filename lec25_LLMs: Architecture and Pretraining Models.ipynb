{"cells":[{"cell_type":"markdown","metadata":{"id":"IGJvTlO79m2R"},"source":["# üôå Welcome"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":475},"executionInfo":{"elapsed":486,"status":"ok","timestamp":1720371504635,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"ncVKzdbH9WNH","outputId":"f53046c8-63c6-4a84-cdd3-e448bb330da1"},"outputs":[{"data":{"text/html":["\n","<div style=\"text-align: center;\">\n","    <div style=\"font-size: 34px; margin-bottom: 20px; color: #8ab4f7;\">Let's get started</div>\n","    <iframe src=\"https://giphy.com/embed/YOjP742CyBbg0zKCbl\" width=\"800\" height=\"392\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n","</div>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","from IPython.display import HTML, display\n","\n","centered_html_with_title = lambda source, title: f\"\"\"\n","<div style=\"text-align: center;\">\n","    <div style=\"font-size: 34px; margin-bottom: 20px; color: #8ab4f7;\">{title}</div>\n","    <iframe src=\"{source}\" width=\"800\" height=\"392\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n","</div>\n","\"\"\"\n","display(HTML(centered_html_with_title(\"https://giphy.com/embed/YOjP742CyBbg0zKCbl\", \"Let's get started\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9LN983Ce9ZIp"},"outputs":[],"source":["# @title\n","from IPython.display import HTML, display, Image, YouTubeVideo\n","from IPython.core.magic import register_line_magic\n","# just for styling pprint function. nothing special\n","\n","def print_html(line, background_color):\n","  display(HTML(f\"\"\"\n","  <div style=\"display: inline-block; font-size:120%; border:1px solid black; padding: 15px; background-color: {background_color}; color: black; margin-bottom: 10px; border-radius:10px; border-width: 1px; border-style: solid; border-color: white; box-sizing: border-box;\">\n","      {line}\n","  </div>\n","  \"\"\"))\n","\n","\n","@register_line_magic\n","def note(line):\n","  print_html(line, \"#94d4f5\")\n","\n","\n","@register_line_magic\n","def warning(line):\n","  print_html(line, \"#f59494\")\n","\n","\n","\n","styled_text_html = lambda x: f\"\"\"\n","<div style=\"padding:20px; color:#150d0a; margin:10px; font-size:220%; text-align:center; display:block; border-radius:20px; border-width: 1px; border-style: solid; border-color: white; background-color: #94d4f5; overflow:hidden;font-weight:500\">\n","{x}</div>\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"8F7_zL0Z98fV"},"source":["# üìç Lecture Plan"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":117},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1717969675441,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"t4gsPNoC92VY","outputId":"ad8dc29f-eb3c-4275-cff4-6186dd38d470"},"outputs":[{"data":{"text/html":["\n","<div style=\"padding:20px; color:#150d0a; margin:10px; font-size:220%; text-align:center; display:block; border-radius:20px; border-width: 1px; border-style: solid; border-color: white; background-color: #94d4f5; overflow:hidden;font-weight:500\">\n","üìç Lecture Plan</div>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","display(HTML(styled_text_html(\"üìç Lecture Plan\")))"]},{"cell_type":"markdown","metadata":{"id":"lHXiztV4BkjL"},"source":["# Key ideas in pretraining\n","\n","- Make sure your model can process large-scale, diverse datasets\n","- Don‚Äôt use labeled data (otherwise you can‚Äôt scale!)\n","- Compute-aware scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717971772363,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"XU_1XiPhDC0B","outputId":"55a49741-a485-45cf-a921-87f49aea92a4"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/g0rpYwqJ/0-c-ZXn-AEouh74p-Cb-Xf.webp\" width=1000 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/g0rpYwqJ/0-c-ZXn-AEouh74p-Cb-Xf.webp\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"elwXwW5VBp1I"},"source":["# Motivating model pretraining from word embeddings"]},{"cell_type":"markdown","metadata":{"id":"3BpSObxilp9P"},"source":["> ‚ÄúYou shall know a word by the company it keeps‚Äù (J. R. Firth 1957: 11)\n","\n","\n","\n","*Consider I **record** the **record**: the two instances of record mean different things.*\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1719340734185,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"0zZ8i37Rre46","outputId":"f2de92bc-a4e4-47da-d503-d7a8a9000bfc"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/LXG8BpKr/Screenshot-2024-06-25-at-22-22-09.png\" width=600 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title **Where we were**: pretrained word embeddings\n","url = \"https://i.postimg.cc/LXG8BpKr/Screenshot-2024-06-25-at-22-22-09.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"_BN5x3kPrfqQ"},"source":["- Start with pretrained word embeddings (no context!)\n","- Learn how to incorporate context in an LSTM or Transformer while training on the task."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1719340684680,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"X1WttQgorfFb","outputId":"327e3d4a-b53c-4b0c-a689-3b8425d429aa"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/wTXjJZWG/Screenshot-2024-06-25-at-22-37-26.png\" width=600 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title **Where we‚Äôre going**: pretraining whole models\n","url = \"https://i.postimg.cc/wTXjJZWG/Screenshot-2024-06-25-at-22-37-26.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"VdqEKvaws2Od"},"source":["- In modern NLP All (or almost all) parameters in NLP networks are initialized via pretraining.\n","- Pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts."]},{"cell_type":"markdown","metadata":{"id":"cszoJTb1tMJP"},"source":["## What can we learn from reconstructing the input?"]},{"cell_type":"markdown","metadata":{"id":"qbAhnNaLtOzj"},"source":["- Tbilisi is the capital city of ___\n","- I went to the ocean to see the fish, turtles, seals, and ___\n","- Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. ___\n","- I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21 ___\n","- The woman walked across the street, checking for traffic over her shoulder.\n","- I put ___ fork down on the table.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1720371512069,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"vjGVeL-5y-3h","outputId":"732d5381-3102-4bbb-876e-1bb598186ca7"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/9M7j7vkH/Screenshot-2024-06-10-at-02-10-41.png\" width=1000 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/9M7j7vkH/Screenshot-2024-06-10-at-02-10-41.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"0Rd67NljEd9A"},"source":["# Pretraining"]},{"cell_type":"markdown","metadata":{"id":"ur7A8plZvkE0"},"source":["language modeling task:\n","- Model $p_\\theta(w_t|w_{1:t-1})$, the probability distribution over words given their past contexts.\n","- There‚Äôs lots of data for this! (In English.) Pretraining through language modeling:\n","- Train a neural network to perform language\n","modeling on a large amount of text.\n","- Save the network parameters in file"]},{"cell_type":"markdown","metadata":{"id":"wOBQthj8yfDs"},"source":["Where does this data come from?\n","\n","- The Pile\n","- RedPajama\n","- RefinedWeb\n","- FineWeb\n","- DataComp-LM [**240 Trillion token**] ü§Ø\n"]},{"cell_type":"markdown","metadata":{"id":"gt_ejBhZzMax"},"source":["# Model pretraining three ways"]},{"cell_type":"markdown","metadata":{"id":"n-9dxq_UzQ-O"},"source":["1. Encoders\n","2. Encoder-Decoders\n","3. Decoders"]},{"cell_type":"markdown","metadata":{"id":"uGL_izQuzSVR"},"source":["## 1. Encoders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1719342760945,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"t9cFDMYTztAf","outputId":"a83f4aa0-529a-4ac2-8887-81de6c6a30c7"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/nh5VtxMM/Screenshot-2024-06-25-at-23-09-57.png\" width=500 height=200></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title Encoder\n","url = \"https://i.postimg.cc/nh5VtxMM/Screenshot-2024-06-25-at-23-09-57.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=500 height=200></div>')\n","display(centered_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1719343687509,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"lp7s1HbR3g-E","outputId":"03c5a7c9-131b-4713-a24d-722980d9117d"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/0N8nxT3R/Screenshot-2024-06-25-at-23-25-08.png\" width=600 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/0N8nxT3R/Screenshot-2024-06-25-at-23-25-08.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"SLhQYmtN3PNQ"},"source":["- So far, we‚Äôve looked at language model pretraining. But encoders get bidirectional context, so we can‚Äôt do language modeling!\n","- Idea: replace some fraction of words in the input with a special [MASK] token; predict these words.\n","\n","h‚ÇÅ, ... , h_T = Encoder(w‚ÇÅ, ... , w_T)\n","y_i ~ Ah_i + b\n","\n","Only add loss terms from words that are \"masked out.\" If xÃÉ is the masked version of x, we're learning p_Œ∏(x|xÃÉ). Called Masked LM."]},{"cell_type":"markdown","metadata":{"id":"QrP5ST-W3xhT"},"source":["**BERT: Bidirectional Encoder Representations from Transformers**\n","\n","Devlin et al., 2018 proposed the ‚ÄúMasked LM‚Äù objective and released the weights of a pretrained Transformer, a model they labeled BERT.\n","\n","Some more details about Masked LM for BERT:\n","- Predict a random 15% of (sub)word tokens.\n","- Replace input word with [MASK] 80% of the time\n","- Replace input word with a random token 10% of the time\n","- Leave input word unchanged 10% of the time (but still predict it!)\n","- Why? Doesn‚Äôt let the model get complacent and not\n","build strong representations of non-masked words.\n","(No masks are seen at fine-tuning time!)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":621},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1719343914464,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"2BQILtBN4F5b","outputId":"d6331742-e020-49a7-bf92-2491109359a9"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/9fFk1wc7/Screenshot-2024-06-25-at-23-29-40.png\" width=700 height=600></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/9fFk1wc7/Screenshot-2024-06-25-at-23-29-40.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=700 height=600></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"vDMgkNpW4Evd"},"source":["Details about BERT\n","\n","Two models were released:\n","- BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params.\n","- BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params.\n","\n","Trained on:\n","- BooksCorpus (800 million words)\n","- English Wikipedia (2,500 million words)\n","\n","Pretraining is expensive and impractical on a single GPU.\n","- BERT was pretrained with 64 TPU chips for a total of 4 days.\n","- (TPUs are special tensor operation acceleration hardware)\n","\n","Finetuning is practical and common on a single GPU\n","- ‚ÄúPretrain once, finetune many times.‚Äù"]},{"cell_type":"markdown","metadata":{"id":"vIYsaLpg41Rt"},"source":["**Limitations of pretrained encoders**\n","\n","Those results looked great! Why not use pretrained encoders for everything?\n","\n","If your task involves generating sequences, consider using a pretrained decoder; BERT and other pretrained encoders don‚Äôt naturally lead to nice autoregressive (1-word-at-a-time) generation methods."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1719344127251,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"Uvf4yx2V4Dl1","outputId":"62e94ac9-6363-4169-d9ad-f08c62d1f104"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/zvcDXYpd/Screenshot-2024-06-25-at-23-33-41.png\" width=1400 height=400></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/zvcDXYpd/Screenshot-2024-06-25-at-23-33-41.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1400 height=400></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"ThYsDPuN4-tq"},"source":["**Extensions of BERT**\n","\n","You‚Äôll see a lot of BERT variants like RoBERTa, SpanBERT, +++\n","\n","Some generally accepted improvements to the BERT pretraining formula:\n","\n","- RoBERTa: mainly just train BERT for longer and remove next sentence prediction!\n","- SpanBERT: masking contiguous spans of words makes a harder, more useful pretraining task"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1719344514296,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"N_SxJjW96umn","outputId":"5f67f53d-1030-4d68-e21c-26e8885317b1"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/7hFvWHKy/Screenshot-2024-06-25-at-23-40-15.png\" width=1000 height=400></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/7hFvWHKy/Screenshot-2024-06-25-at-23-40-15.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=400></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"pqQci89c6SgC"},"source":["A takeaway from the RoBERTa paper:\n","- more compute\n","- more data can improve pretraining even when not changing the underlying Transformer encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"elapsed":368,"status":"ok","timestamp":1719342803150,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"zZ1z0CzTzzJO","outputId":"27f9b939-e71e-4668-e4c1-444a78d83d9f"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/B6fqXCT0/Screenshot-2024-06-25-at-23-10-00.png\" width=500 height=300></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title Encoder - Decoders\n","url = \"https://i.postimg.cc/B6fqXCT0/Screenshot-2024-06-25-at-23-10-00.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=500 height=300></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"x9Fue49Y69Qf"},"source":["For encoder-decoders, we could do something like language modeling, but where a prefix of every input is provided to the encoder and is not predicted.\n","\n","The encoder portion can benefit from bidirectional context; the decoder portion is used to train the whole model through language modeling, autoregressively predicting and then conditioning on one token at a time."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":497,"status":"ok","timestamp":1719344760831,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"TL6PjXpj7JcE","outputId":"9e33c4cc-dec7-49c6-98e8-656cf809cdc0"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/BQFf1FvZ/Screenshot-2024-06-25-at-23-42-52.png\" width=600 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/BQFf1FvZ/Screenshot-2024-06-25-at-23-42-52.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1719342847250,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"Do0h9AWozz1j","outputId":"077c5de1-9f98-4224-c62d-15d219e8d7cd"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/pVZ2Lzyw/Screenshot-2024-06-25-at-23-10-03.png\" width=500 height=200></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title Decoders\n","url = \"https://i.postimg.cc/pVZ2Lzyw/Screenshot-2024-06-25-at-23-10-03.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=500 height=200></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"drvDzMiL9Sa4"},"source":["It‚Äôs natural to pretrain decoders as language models and then\n","use them as generators, finetuning their\n","\n","This is helpful in tasks where the output is a sequence with a vocabulary like that at\n","\n","pretraining time!\n","- Dialogue (context=dialogue history)\n","- Summarization (context=document)"]},{"cell_type":"markdown","metadata":{"id":"gc3OFlvf9aZ6"},"source":["# GPT"]},{"cell_type":"markdown","metadata":{"id":"lZpQjwG_9aSX"},"source":["2018‚Äôs GPT was a big success in pretraining a decoder!\n","\n","- Transformer decoder with 12 layers, 117M parameters.\n","- 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers.\n","- Byte-pair encoding with 40,000 merges\n","- Trained on BooksCorpus: over 7000 unique books.\n","- Contains long spans of contiguous text, for learning long-distance dependencies.\n","- The acronym ‚ÄúGPT‚Äù never showed up in the original paper; it could stand for ‚ÄúGenerative PreTraining‚Äù or ‚ÄúGenerative Pretrained Transformer‚Äù"]},{"cell_type":"markdown","metadata":{"id":"qyc8zRg69qVc"},"source":["# GPT 2"]},{"cell_type":"markdown","metadata":{"id":"hzvxspk29sJi"},"source":[]},{"cell_type":"markdown","metadata":{"id":"XvIg5-dZ9srl"},"source":["# GPT 3"]},{"cell_type":"markdown","metadata":{"id":"xzy5bt1x9sFP"},"source":["So far, we‚Äôve interacted with pretrained models in two ways:\n","- Sample from the distributions they define (maybe providing a prompt)\n","- Fine-tune them on a task we care about and take their predictions.\n","\n","Very large language models seem to perform some kind of learning without gradient\n","steps simply from examples you provide within their contexts.\n","\n","GPT-3 is the canonical example of this. The largest T5 model had 11 billion parameters.\n","GPT-3 has 175 billion parameters.\n","\n","ChatGPT/GPT-4/GPT-3.5 Turbo introduced a further instruction-tuning idea that we\n","cover next lecture"]},{"cell_type":"markdown","metadata":{"id":"8CweAgYO-HDk"},"source":["# Scaling Efficiency"]},{"cell_type":"markdown","metadata":{"id":"mzfho5et-Idf"},"source":["GPT-3 was 175B parameters and trained on 300B tokens of text.\n","\n","Roughly, the cost of training a large transformer scales as parameters*tokens\n","\n","Did OpenAI strike the right parameter-token data to get the best model? No."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"elapsed":4,"status":"error","timestamp":1720371493074,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"noA-cjy--oey","outputId":"7e12b7eb-ba0d-4ad8-9993-19165ddd2514"},"outputs":[{"ename":"NameError","evalue":"name 'HTML' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2a38196b617f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://i.postimg.cc/3RWxMwFy/Screenshot-2024-06-25-at-23-58-23.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcentered_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'<div style=\"text-align:center;\"><img src=\"{url}\" width=900 height=300></div>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentered_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"]}],"source":["# @title\n","url = \"https://i.postimg.cc/3RWxMwFy/Screenshot-2024-06-25-at-23-58-23.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=900 height=300></div>')\n","display(centered_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1719346948594,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"XPpiBVM8D2eU","outputId":"03b19180-f376-4dc5-d145-4bea1b65d055"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/d3jtJhDw/Screenshot-2024-06-26-at-00-18-57.png\" width=1000 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/d3jtJhDw/Screenshot-2024-06-26-at-00-18-57.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1719346971988,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"Sm6cZzeJD-xS","outputId":"212b4e7b-1e42-43ff-ade2-9f02f368e8db"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/L6m8N30X/Screenshot-2024-06-26-at-00-18-45.png\" width=1200 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/L6m8N30X/Screenshot-2024-06-26-at-00-18-45.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1200 height=500></div>')\n","display(centered_image)"]},{"cell_type":"markdown","metadata":{"id":"kuxCJ9J9D1U_"},"source":["# What kinds of things does pretraining teach?"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717971973759,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"jvK-kDqhG9Hm","outputId":"062e98eb-99fe-4da2-8438-f14fe47399da"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/LXfrXjqH/Screenshot-2024-06-10-at-02-13-41.png\" width=1000 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/LXfrXjqH/Screenshot-2024-06-10-at-02-13-41.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717971985541,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"jTkHvNd4G_N-","outputId":"cb3f4a3a-d845-41c8-bbbd-21c0ece9307b"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/2jQsNFRG/Screenshot-2024-06-10-at-02-15-32.png\" width=1000 height=500></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","url = \"https://i.postimg.cc/2jQsNFRG/Screenshot-2024-06-10-at-02-15-32.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717972269334,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"},"user_tz":-240},"id":"fH8iFGdnHCGX","outputId":"688fc801-0af3-4afb-d730-5c24d104bfe9"},"outputs":[{"data":{"text/html":["<div style=\"text-align:center;\"><iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/zjkBMFhNj_g\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# @title\n","video_id = \"zjkBMFhNj_g\"  # Replace with the actual YouTube video ID\n","\n","embed_code = f'<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/{video_id}\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n","\n","centered_video = HTML(f'<div style=\"text-align:center;\">{embed_code}</div>')\n","display(centered_video)"]},{"cell_type":"markdown","metadata":{"id":"F2kDbV6aj3Lk"},"source":["# asd\n","\n","- modern llm pretraining\n","- in context learning / chain of thought\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HouE21lbz5O"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPWnuIpjMwYDQUHB5Lqa0MA","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
