{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1d4e8f",
   "metadata": {},
   "source": [
    "# Quiz: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff066a",
   "metadata": {},
   "source": [
    "### Multiple Choice Questions\n",
    "\n",
    "1. What is the main goal of word embeddings?  \n",
    "a) To translate text into another language  \n",
    "b) To convert words into dense, meaningful numerical vectors  \n",
    "c) To apply grammar rules  \n",
    "d) To increase vocabulary size\n",
    "\n",
    "2. Which of the following is a **dense** representation of text?  \n",
    "a) One-hot vector  \n",
    "b) Bag-of-words  \n",
    "c) TF-IDF  \n",
    "d) Word2Vec\n",
    "\n",
    "3. Which method learns word embeddings by predicting surrounding words in a sentence?  \n",
    "a) CBOW  \n",
    "b) Skip-gram  \n",
    "c) LSTM  \n",
    "d) Transformer\n",
    "\n",
    "4. What does cosine similarity measure in the context of word vectors?  \n",
    "a) Angle between vectors  \n",
    "b) Word frequency  \n",
    "c) Euclidean distance  \n",
    "d) Probability of occurrence\n",
    "\n",
    "5. What is a limitation of static word embeddings like Word2Vec and GloVe?  \n",
    "a) They are too fast to train  \n",
    "b) They assign the same vector to every instance of a word regardless of context  \n",
    "c) They cannot be used with RNNs  \n",
    "d) They only work on images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0b802",
   "metadata": {},
   "source": [
    "### Analytical Questions\n",
    "\n",
    "1. Why do one-hot vectors fail to capture semantic meaning?\n",
    "\n",
    "2. What does it mean when we say two words are \"close\" in embedding space?\n",
    "\n",
    "3. How can bias in training data affect word embeddings? What are ways to detect or mitigate this?\n",
    "\n",
    "4. Explain how word embeddings can help downstream tasks like classification or translation.\n",
    "\n",
    "5. Compare and contrast CBOW and Skip-gram in terms of their objectives and behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1145f4d",
   "metadata": {},
   "source": [
    "# Assignment: Exploring and Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b26475",
   "metadata": {},
   "source": [
    "### Task 1: Load Pretrained Word Embeddings\n",
    "\n",
    "- Load GloVe embeddings (e.g., `glove.6B.100d.txt`)\n",
    "- Write a function to find the most similar words to a given word using cosine similarity\n",
    "- Test your function on several example words (e.g., “king”, “car”, “university”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (starter)\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def load_glove(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def most_similar(word, embeddings, top_n=5):\n",
    "    if word not in embeddings:\n",
    "        return []\n",
    "    word_vec = embeddings[word]\n",
    "    sims = {}\n",
    "    for other, vec in embeddings.items():\n",
    "        if other == word:\n",
    "            continue\n",
    "        cos_sim = np.dot(word_vec, vec) / (norm(word_vec) * norm(vec))\n",
    "        sims[other] = cos_sim\n",
    "    return sorted(sims.items(), key=lambda item: item[1], reverse=True)[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b640977",
   "metadata": {},
   "source": [
    "### Task 2: Visualize Word Embeddings\n",
    "\n",
    "- Pick 50–100 common words (e.g., animals, professions, places)\n",
    "- Use t-SNE or PCA to reduce embedding dimensions to 2D\n",
    "- Plot them using matplotlib and analyze clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eef567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (starter)\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define your word list and extract vectors using glove\n",
    "# words = [...]\n",
    "# vectors = [embeddings[word] for word in words]\n",
    "\n",
    "# tsne = TSNE(n_components=2, random_state=0)\n",
    "# reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# for i, word in enumerate(words):\n",
    "#     plt.scatter(reduced[i, 0], reduced[i, 1])\n",
    "#     plt.annotate(word, (reduced[i, 0], reduced[i, 1]))\n",
    "# plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
