{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "cells": [{"id": "b858bf82", "cell_type": "markdown", "source": "# Quiz: Optimization Techniques", "metadata": {}}, {"id": "c6d6403d", "cell_type": "markdown", "source": "### Multiple Choice Questions\n\n1. What is the key principle behind Gradient Descent?  \na) Increasing the loss function value  \nb) Minimizing model weights  \nc) Iteratively moving in the direction of the steepest descent of the loss function  \nd) Computing gradients with respect to input features\n\n2. What problem does Momentum aim to solve in standard gradient descent?  \na) Large batch sizes  \nb) Oscillations in ravines or sharp curvature  \nc) Missing gradients  \nd) Overfitting\n\n3. Which optimizer accumulates squared gradients to adjust the learning rate per parameter?  \na) SGD  \nb) Momentum  \nc) RMSprop  \nd) AdamW\n\n4. Adam optimizer is a combination of which two methods?  \na) RMSprop and SGD  \nb) Momentum and Adagrad  \nc) Momentum and RMSprop  \nd) RMSprop and SGD with weight decay\n\n5. What happens if the learning rate is too small?  \na) The model may diverge  \nb) Training will converge slowly  \nc) Overfitting occurs immediately  \nd) The loss increases sharply\n\n6. Why does RMSprop divide the learning rate by the root of squared gradients?  \na) To increase the step size  \nb) To prevent vanishing gradients  \nc) To normalize updates and stabilize learning  \nd) To introduce noise\n\n7. Which optimizer is most commonly used as a default in deep learning tasks today?  \na) SGD  \nb) Adagrad  \nc) RMSprop  \nd) Adam\n\n8. What is the effect of setting the beta values too high in Adam?  \na) Gradients explode  \nb) Momentum term vanishes  \nc) Slow adaptation and poor convergence  \nd) Loss instantly becomes zero\n\n9. In which scenario might plain SGD outperform adaptive optimizers?  \na) Complex NLP tasks  \nb) Low-resource environments  \nc) Large vision models with good initialization  \nd) Transformer-based architectures\n\n10. What can cause an optimizer to overshoot the optimal point during training?  \na) Low learning rate  \nb) High batch size  \nc) High learning rate  \nd) Low momentum\n", "metadata": {}}, {"id": "0ba375e5", "cell_type": "markdown", "source": "### Analytical Questions\n\n1. Describe the advantage of using momentum in optimization. How does it affect convergence in valleys or ravines?\n\n2. Compare the adaptive behavior of RMSprop and Adam. What trade-offs exist between them?\n\n3. Explain a scenario where Gradient Descent might fail without additional techniques like momentum or adaptive learning.\n\n4. What considerations would you have when selecting an optimizer for a model trained on noisy or sparse data?\n\n5. How can visualizing the loss surface help in understanding optimizer behavior?\n\n6. Suppose your training loss oscillates but never decreases. Which optimizer settings might you investigate and why?\n\n7. What\u2019s the purpose of using bias correction in Adam optimizer\u2019s early steps?\n\n8. Discuss the impact of mini-batch size on optimization stability and convergence.\n\n9. If your model converges too quickly to a suboptimal solution, what optimizer modifications might help?\n\n10. How would you evaluate which optimizer works best for a given problem beyond just final accuracy?\n", "metadata": {}}]}