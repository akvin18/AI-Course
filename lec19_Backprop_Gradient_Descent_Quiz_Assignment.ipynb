{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Analytical Quiz: Backpropagation & Gradient Descent"}, {"cell_type": "markdown", "metadata": {}, "source": "### Questions\n\n1. In your own words, explain the goal of gradient descent. Why is it used in training neural networks?\n\n2. A model\u2019s loss decreases very slowly during training. What could be the causes and how would you troubleshoot this?\n\n3. What is the role of the learning rate in gradient descent? Describe what might happen if it\u2019s too small or too large.\n\n4. How does backpropagation use the chain rule from calculus? Why is this important?\n\n5. Suppose you are training a neural network and you notice the gradients are either vanishing or exploding. What are some ways to address this?\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# Assignment: Simulating Gradient Descent and Backpropagation"}, {"cell_type": "markdown", "metadata": {}, "source": "### Task 1: Visualize Gradient Descent\n\n- Plot the function `f(x) = x\u00b2 + 4` and simulate one-variable gradient descent.\n- Use a starting point like `x = 10`.\n- At each step, update `x = x - learning_rate * gradient`.\n- Visualize how the function value decreases over iterations.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Your code here\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = 10\nlr = 0.1\nhistory = []\n\nfor _ in range(30):\n    grad = 2 * x  # derivative of f(x) = x\u00b2 + 4\n    x = x - lr * grad\n    history.append((x, x**2 + 4))\n\nxs, ys = zip(*history)\nplt.plot(xs, ys, marker='o')\nplt.title(\"Gradient Descent on f(x) = x\u00b2 + 4\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.grid(True)\nplt.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Task 2: Manual Backpropagation (2-layer NN)\n\n- Implement a simple neural network with 1 hidden layer (no libraries).\n- Input: x = 1.0, weight1 = 0.5, weight2 = -1.0, learning rate = 0.1\n- Target output: 0.5\n- Use sigmoid as activation and MSE as the loss function\n- Manually compute forward pass, gradients, and one update step\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "# Your code here\n# Define sigmoid and derivative\nsigmoid = lambda x: 1 / (1 + np.exp(-x))\nsigmoid_prime = lambda x: sigmoid(x) * (1 - sigmoid(x))\n\n# Forward pass\nx = 1.0\nw1 = 0.5\nw2 = -1.0\ny_true = 0.5\n\nh = sigmoid(x * w1)\ny_pred = sigmoid(h * w2)\n\n# Loss\nloss = 0.5 * (y_true - y_pred)**2\nprint(\"Loss before update:\", loss)\n\n# Backward pass\ndL_dy = y_pred - y_true\ndy_dz2 = sigmoid_prime(h * w2)\ndz2_dw2 = h\ndz2_dh = w2\n\ndh_dz1 = sigmoid_prime(x * w1)\ndz1_dw1 = x\n\n# Gradients\ndL_dw2 = dL_dy * dy_dz2 * dz2_dw2\ndL_dw1 = dL_dy * dy_dz2 * dz2_dh * dh_dz1 * dz1_dw1\n\n# Update\nw1 -= 0.1 * dL_dw1\nw2 -= 0.1 * dL_dw2\n\nprint(\"Updated weights:\", w1, w2)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}