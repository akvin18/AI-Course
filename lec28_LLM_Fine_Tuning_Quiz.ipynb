{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "04726299", "cell_type": "markdown", "source": "# Quiz: Techniques of Fine-Tuning Large Language Models (LLMs)", "metadata": {}}, {"id": "fde2cbae", "cell_type": "markdown", "source": "### Multiple Choice Questions\n\n1. What is the main goal of fine-tuning an LLM?  \na) Reduce training time  \nb) Adapt the model to new, specific tasks or domains  \nc) Improve model compression  \nd) Generate embeddings\n\n2. Which technique allows fine-tuning with a small number of additional trainable parameters?  \na) Full model fine-tuning  \nb) Pruning  \nc) Low-Rank Adaptation (LoRA)  \nd) Gradient clipping\n\n3. What does QLoRA add to LoRA to reduce memory usage during training?  \na) Mixed precision  \nb) 8-bit quantization  \nc) Parameter freezing  \nd) Gradient accumulation\n\n4. What is a potential downside of full fine-tuning on a small dataset?  \na) It saves compute  \nb) It generalizes better  \nc) It requires fewer parameters  \nd) It can lead to overfitting\n\n5. Which of the following is an advantage of adapter-based fine-tuning?  \na) Requires no base model  \nb) Freezes the entire model except for added layers  \nc) Increases training data requirements  \nd) Requires gradient accumulation for all layers\n\n6. What is the purpose of freezing layers during fine-tuning?  \na) Prevent overfitting and save computation  \nb) Improve generalization on all domains  \nc) Reduce the model size permanently  \nd) Randomize weight updates\n\n7. LoRA assumes that the weight updates in transformers are:  \na) Large and sparse  \nb) Full-rank and dense  \nc) Low-rank and efficient  \nd) Non-parametric\n\n8. What happens in instruction tuning of LLMs?  \na) The model is pre-trained again from scratch  \nb) The model learns to follow user directions using labeled prompt-response pairs  \nc) The model is pruned aggressively  \nd) The model is quantized to 4-bit\n\n9. What is the key trade-off when using quantized LLMs for fine-tuning?  \na) More accuracy, less memory  \nb) Less compute, more training time  \nc) Lower memory usage, slightly reduced performance  \nd) None \u2014 quantization always improves models\n\n10. Which of the following is a good reason to use parameter-efficient fine-tuning methods?  \na) They give higher accuracy than full fine-tuning  \nb) They support model deployment on GPU clusters  \nc) They reduce the number of trainable parameters and enable reuse across tasks  \nd) They are required for pretraining\n", "metadata": {}}, {"id": "763c02b0", "cell_type": "markdown", "source": "### Analytical Questions\n\n1. Compare full fine-tuning with LoRA. When would you use one over the other?\n\n2. How does freezing most model layers help in the context of domain adaptation?\n\n3. Imagine you want to adapt an LLM to customer support for a specific company. Which fine-tuning technique would you use and why?\n\n4. Explain how QLoRA combines quantization and LoRA to make fine-tuning affordable.\n\n5. What kinds of downstream tasks benefit most from instruction tuning?\n\n6. How would you evaluate whether fine-tuning actually improved your model's task-specific performance?\n\n7. What risks are involved when fine-tuning a large model with very little data?\n\n8. Describe a scenario where adapter-based fine-tuning is preferable to LoRA.\n\n9. What are the challenges in deploying fine-tuned LLMs to edge devices or mobile platforms?\n\n10. How can you reuse a LoRA fine-tuned model across multiple downstream tasks?\n", "metadata": {}}]}