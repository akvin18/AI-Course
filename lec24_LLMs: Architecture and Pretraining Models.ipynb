{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWnuIpjMwYDQUHB5Lqa0MA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üôå Welcome"],"metadata":{"id":"IGJvTlO79m2R"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":475},"cellView":"form","id":"ncVKzdbH9WNH","executionInfo":{"status":"ok","timestamp":1720371504635,"user_tz":-240,"elapsed":486,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"f53046c8-63c6-4a84-cdd3-e448bb330da1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<div style=\"text-align: center;\">\n","    <div style=\"font-size: 34px; margin-bottom: 20px; color: #8ab4f7;\">Let's get started</div>\n","    <iframe src=\"https://giphy.com/embed/YOjP742CyBbg0zKCbl\" width=\"800\" height=\"392\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n","</div>\n"]},"metadata":{}}],"source":["# @title\n","from IPython.display import HTML, display\n","\n","centered_html_with_title = lambda source, title: f\"\"\"\n","<div style=\"text-align: center;\">\n","    <div style=\"font-size: 34px; margin-bottom: 20px; color: #8ab4f7;\">{title}</div>\n","    <iframe src=\"{source}\" width=\"800\" height=\"392\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n","</div>\n","\"\"\"\n","display(HTML(centered_html_with_title(\"https://giphy.com/embed/YOjP742CyBbg0zKCbl\", \"Let's get started\")))"]},{"cell_type":"code","source":["# @title\n","from IPython.display import HTML, display, Image, YouTubeVideo\n","from IPython.core.magic import register_line_magic\n","# just for styling pprint function. nothing special\n","\n","def print_html(line, background_color):\n","  display(HTML(f\"\"\"\n","  <div style=\"display: inline-block; font-size:120%; border:1px solid black; padding: 15px; background-color: {background_color}; color: black; margin-bottom: 10px; border-radius:10px; border-width: 1px; border-style: solid; border-color: white; box-sizing: border-box;\">\n","      {line}\n","  </div>\n","  \"\"\"))\n","\n","\n","@register_line_magic\n","def note(line):\n","  print_html(line, \"#94d4f5\")\n","\n","\n","@register_line_magic\n","def warning(line):\n","  print_html(line, \"#f59494\")\n","\n","\n","\n","styled_text_html = lambda x: f\"\"\"\n","<div style=\"padding:20px; color:#150d0a; margin:10px; font-size:220%; text-align:center; display:block; border-radius:20px; border-width: 1px; border-style: solid; border-color: white; background-color: #94d4f5; overflow:hidden;font-weight:500\">\n","{x}</div>\n","\"\"\""],"metadata":{"cellView":"form","id":"9LN983Ce9ZIp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üìç Lecture Plan"],"metadata":{"id":"8F7_zL0Z98fV"}},{"cell_type":"code","source":["# @title\n","display(HTML(styled_text_html(\"üìç Lecture Plan\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117},"cellView":"form","id":"t4gsPNoC92VY","executionInfo":{"status":"ok","timestamp":1717969675441,"user_tz":-240,"elapsed":338,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"ad8dc29f-eb3c-4275-cff4-6186dd38d470"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<div style=\"padding:20px; color:#150d0a; margin:10px; font-size:220%; text-align:center; display:block; border-radius:20px; border-width: 1px; border-style: solid; border-color: white; background-color: #94d4f5; overflow:hidden;font-weight:500\">\n","üìç Lecture Plan</div>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Key ideas in pretraining\n","\n","- Make sure your model can process large-scale, diverse datasets\n","- Don‚Äôt use labeled data (otherwise you can‚Äôt scale!)\n","- Compute-aware scaling"],"metadata":{"id":"lHXiztV4BkjL"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/g0rpYwqJ/0-c-ZXn-AEouh74p-Cb-Xf.webp\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"XU_1XiPhDC0B","executionInfo":{"status":"ok","timestamp":1717971772363,"user_tz":-240,"elapsed":4,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"55a49741-a485-45cf-a921-87f49aea92a4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/g0rpYwqJ/0-c-ZXn-AEouh74p-Cb-Xf.webp\" width=1000 height=500></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Motivating model pretraining from word embeddings"],"metadata":{"id":"elwXwW5VBp1I"}},{"cell_type":"markdown","source":["> ‚ÄúYou shall know a word by the company it keeps‚Äù (J. R. Firth 1957: 11)\n","\n","\n","\n","*Consider I **record** the **record**: the two instances of record mean different things.*\n","\n","\n","\n"],"metadata":{"id":"3BpSObxilp9P"}},{"cell_type":"code","source":["# @title **Where we were**: pretrained word embeddings\n","url = \"https://i.postimg.cc/LXG8BpKr/Screenshot-2024-06-25-at-22-22-09.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"0zZ8i37Rre46","executionInfo":{"status":"ok","timestamp":1719340734185,"user_tz":-240,"elapsed":2,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"f2de92bc-a4e4-47da-d503-d7a8a9000bfc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/LXG8BpKr/Screenshot-2024-06-25-at-22-22-09.png\" width=600 height=500></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["- Start with pretrained word embeddings (no context!)\n","- Learn how to incorporate context in an LSTM or Transformer while training on the task."],"metadata":{"id":"_BN5x3kPrfqQ"}},{"cell_type":"code","source":["# @title **Where we‚Äôre going**: pretraining whole models\n","url = \"https://i.postimg.cc/wTXjJZWG/Screenshot-2024-06-25-at-22-37-26.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"X1WttQgorfFb","executionInfo":{"status":"ok","timestamp":1719340684680,"user_tz":-240,"elapsed":355,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"327e3d4a-b53c-4b0c-a689-3b8425d429aa"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/wTXjJZWG/Screenshot-2024-06-25-at-22-37-26.png\" width=600 height=500></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["- In modern NLP All (or almost all) parameters in NLP networks are initialized via pretraining.\n","- Pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts."],"metadata":{"id":"VdqEKvaws2Od"}},{"cell_type":"markdown","source":["## What can we learn from reconstructing the input?"],"metadata":{"id":"cszoJTb1tMJP"}},{"cell_type":"markdown","source":["- Tbilisi is the capital city of ___\n","- I went to the ocean to see the fish, turtles, seals, and ___\n","- Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. ___\n","- I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21 ___\n","- The woman walked across the street, checking for traffic over her shoulder.\n","- I put ___ fork down on the table.\n"],"metadata":{"id":"qbAhnNaLtOzj"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/9M7j7vkH/Screenshot-2024-06-10-at-02-10-41.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"vjGVeL-5y-3h","executionInfo":{"status":"ok","timestamp":1720371512069,"user_tz":-240,"elapsed":455,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"732d5381-3102-4bbb-876e-1bb598186ca7"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/9M7j7vkH/Screenshot-2024-06-10-at-02-10-41.png\" width=1000 height=500></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Pretraining"],"metadata":{"id":"0Rd67NljEd9A"}},{"cell_type":"markdown","source":["language modeling task:\n","- Model $p_\\theta(w_t|w_{1:t-1})$, the probability distribution over words given their past contexts.\n","- There‚Äôs lots of data for this! (In English.) Pretraining through language modeling:\n","- Train a neural network to perform language\n","modeling on a large amount of text.\n","- Save the network parameters in file"],"metadata":{"id":"ur7A8plZvkE0"}},{"cell_type":"markdown","source":["Where does this data come from?\n","\n","- The Pile\n","- RedPajama\n","- RefinedWeb\n","- FineWeb\n","- DataComp-LM [**240 Trillion token**] ü§Ø\n"],"metadata":{"id":"wOBQthj8yfDs"}},{"cell_type":"markdown","source":["# Model pretraining three ways"],"metadata":{"id":"gt_ejBhZzMax"}},{"cell_type":"markdown","source":["1. Encoders\n","2. Encoder-Decoders\n","3. Decoders"],"metadata":{"id":"n-9dxq_UzQ-O"}},{"cell_type":"markdown","source":["## 1. Encoders"],"metadata":{"id":"uGL_izQuzSVR"}},{"cell_type":"code","source":["# @title Encoder\n","url = \"https://i.postimg.cc/nh5VtxMM/Screenshot-2024-06-25-at-23-09-57.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=500 height=200></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"t9cFDMYTztAf","executionInfo":{"status":"ok","timestamp":1719342760945,"user_tz":-240,"elapsed":338,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"a83f4aa0-529a-4ac2-8887-81de6c6a30c7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/nh5VtxMM/Screenshot-2024-06-25-at-23-09-57.png\" width=500 height=200></div>"]},"metadata":{}}]},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/0N8nxT3R/Screenshot-2024-06-25-at-23-25-08.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"lp7s1HbR3g-E","executionInfo":{"status":"ok","timestamp":1719343687509,"user_tz":-240,"elapsed":5,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"03c5a7c9-131b-4713-a24d-722980d9117d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/0N8nxT3R/Screenshot-2024-06-25-at-23-25-08.png\" width=600 height=500></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["- So far, we‚Äôve looked at language model pretraining. But encoders get bidirectional context, so we can‚Äôt do language modeling!\n","- Idea: replace some fraction of words in the input with a special [MASK] token; predict these words.\n","\n","h‚ÇÅ, ... , h_T = Encoder(w‚ÇÅ, ... , w_T)\n","y_i ~ Ah_i + b\n","\n","Only add loss terms from words that are \"masked out.\" If xÃÉ is the masked version of x, we're learning p_Œ∏(x|xÃÉ). Called Masked LM."],"metadata":{"id":"SLhQYmtN3PNQ"}},{"cell_type":"markdown","source":["**BERT: Bidirectional Encoder Representations from Transformers**\n","\n","Devlin et al., 2018 proposed the ‚ÄúMasked LM‚Äù objective and released the weights of a pretrained Transformer, a model they labeled BERT.\n","\n","Some more details about Masked LM for BERT:\n","- Predict a random 15% of (sub)word tokens.\n","- Replace input word with [MASK] 80% of the time\n","- Replace input word with a random token 10% of the time\n","- Leave input word unchanged 10% of the time (but still predict it!)\n","- Why? Doesn‚Äôt let the model get complacent and not\n","build strong representations of non-masked words.\n","(No masks are seen at fine-tuning time!)"],"metadata":{"id":"QrP5ST-W3xhT"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/9fFk1wc7/Screenshot-2024-06-25-at-23-29-40.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=700 height=600></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":621},"cellView":"form","id":"2BQILtBN4F5b","executionInfo":{"status":"ok","timestamp":1719343914464,"user_tz":-240,"elapsed":2,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"d6331742-e020-49a7-bf92-2491109359a9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/9fFk1wc7/Screenshot-2024-06-25-at-23-29-40.png\" width=700 height=600></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Details about BERT\n","\n","Two models were released:\n","- BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params.\n","- BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params.\n","\n","Trained on:\n","- BooksCorpus (800 million words)\n","- English Wikipedia (2,500 million words)\n","\n","Pretraining is expensive and impractical on a single GPU.\n","- BERT was pretrained with 64 TPU chips for a total of 4 days.\n","- (TPUs are special tensor operation acceleration hardware)\n","\n","Finetuning is practical and common on a single GPU\n","- ‚ÄúPretrain once, finetune many times.‚Äù"],"metadata":{"id":"vDMgkNpW4Evd"}},{"cell_type":"markdown","source":["**Limitations of pretrained encoders**\n","\n","Those results looked great! Why not use pretrained encoders for everything?\n","\n","If your task involves generating sequences, consider using a pretrained decoder; BERT and other pretrained encoders don‚Äôt naturally lead to nice autoregressive (1-word-at-a-time) generation methods."],"metadata":{"id":"vIYsaLpg41Rt"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/zvcDXYpd/Screenshot-2024-06-25-at-23-33-41.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1400 height=400></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"cellView":"form","id":"Uvf4yx2V4Dl1","executionInfo":{"status":"ok","timestamp":1719344127251,"user_tz":-240,"elapsed":346,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"62e94ac9-6363-4169-d9ad-f08c62d1f104"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/zvcDXYpd/Screenshot-2024-06-25-at-23-33-41.png\" width=1400 height=400></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["**Extensions of BERT**\n","\n","You‚Äôll see a lot of BERT variants like RoBERTa, SpanBERT, +++\n","\n","Some generally accepted improvements to the BERT pretraining formula:\n","\n","- RoBERTa: mainly just train BERT for longer and remove next sentence prediction!\n","- SpanBERT: masking contiguous spans of words makes a harder, more useful pretraining task"],"metadata":{"id":"ThYsDPuN4-tq"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/7hFvWHKy/Screenshot-2024-06-25-at-23-40-15.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=400></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"cellView":"form","id":"N_SxJjW96umn","executionInfo":{"status":"ok","timestamp":1719344514296,"user_tz":-240,"elapsed":406,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"5f67f53d-1030-4d68-e21c-26e8885317b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/7hFvWHKy/Screenshot-2024-06-25-at-23-40-15.png\" width=1000 height=400></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["A takeaway from the RoBERTa paper:\n","- more compute\n","- more data can improve pretraining even when not changing the underlying Transformer encoder."],"metadata":{"id":"pqQci89c6SgC"}},{"cell_type":"code","source":["# @title Encoder - Decoders\n","url = \"https://i.postimg.cc/B6fqXCT0/Screenshot-2024-06-25-at-23-10-00.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=500 height=300></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"cellView":"form","id":"zZ1z0CzTzzJO","executionInfo":{"status":"ok","timestamp":1719342803150,"user_tz":-240,"elapsed":368,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"27f9b939-e71e-4668-e4c1-444a78d83d9f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/B6fqXCT0/Screenshot-2024-06-25-at-23-10-00.png\" width=500 height=300></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["For encoder-decoders, we could do something like language modeling, but where a prefix of every input is provided to the encoder and is not predicted.\n","\n","The encoder portion can benefit from bidirectional context; the decoder portion is used to train the whole model through language modeling, autoregressively predicting and then conditioning on one token at a time."],"metadata":{"id":"x9Fue49Y69Qf"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/BQFf1FvZ/Screenshot-2024-06-25-at-23-42-52.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=600 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"TL6PjXpj7JcE","executionInfo":{"status":"ok","timestamp":1719344760831,"user_tz":-240,"elapsed":497,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"9e33c4cc-dec7-49c6-98e8-656cf809cdc0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/BQFf1FvZ/Screenshot-2024-06-25-at-23-42-52.png\" width=600 height=500></div>"]},"metadata":{}}]},{"cell_type":"code","source":["# @title Decoders\n","url = \"https://i.postimg.cc/pVZ2Lzyw/Screenshot-2024-06-25-at-23-10-03.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=500 height=200></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"cellView":"form","id":"Do0h9AWozz1j","executionInfo":{"status":"ok","timestamp":1719342847250,"user_tz":-240,"elapsed":2,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"077c5de1-9f98-4224-c62d-15d219e8d7cd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/pVZ2Lzyw/Screenshot-2024-06-25-at-23-10-03.png\" width=500 height=200></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["It‚Äôs natural to pretrain decoders as language models and then\n","use them as generators, finetuning their\n","\n","This is helpful in tasks where the output is a sequence with a vocabulary like that at\n","\n","pretraining time!\n","- Dialogue (context=dialogue history)\n","- Summarization (context=document)"],"metadata":{"id":"drvDzMiL9Sa4"}},{"cell_type":"markdown","source":["# GPT"],"metadata":{"id":"gc3OFlvf9aZ6"}},{"cell_type":"markdown","source":["2018‚Äôs GPT was a big success in pretraining a decoder!\n","\n","- Transformer decoder with 12 layers, 117M parameters.\n","- 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers.\n","- Byte-pair encoding with 40,000 merges\n","- Trained on BooksCorpus: over 7000 unique books.\n","- Contains long spans of contiguous text, for learning long-distance dependencies.\n","- The acronym ‚ÄúGPT‚Äù never showed up in the original paper; it could stand for ‚ÄúGenerative PreTraining‚Äù or ‚ÄúGenerative Pretrained Transformer‚Äù"],"metadata":{"id":"lZpQjwG_9aSX"}},{"cell_type":"markdown","source":["# GPT 2"],"metadata":{"id":"qyc8zRg69qVc"}},{"cell_type":"markdown","source":[],"metadata":{"id":"hzvxspk29sJi"}},{"cell_type":"markdown","source":["# GPT 3"],"metadata":{"id":"XvIg5-dZ9srl"}},{"cell_type":"markdown","source":["So far, we‚Äôve interacted with pretrained models in two ways:\n","- Sample from the distributions they define (maybe providing a prompt)\n","- Fine-tune them on a task we care about and take their predictions.\n","\n","Very large language models seem to perform some kind of learning without gradient\n","steps simply from examples you provide within their contexts.\n","\n","GPT-3 is the canonical example of this. The largest T5 model had 11 billion parameters.\n","GPT-3 has 175 billion parameters.\n","\n","ChatGPT/GPT-4/GPT-3.5 Turbo introduced a further instruction-tuning idea that we\n","cover next lecture"],"metadata":{"id":"xzy5bt1x9sFP"}},{"cell_type":"markdown","source":["# Scaling Efficiency"],"metadata":{"id":"8CweAgYO-HDk"}},{"cell_type":"markdown","source":["GPT-3 was 175B parameters and trained on 300B tokens of text.\n","\n","Roughly, the cost of training a large transformer scales as parameters*tokens\n","\n","Did OpenAI strike the right parameter-token data to get the best model? No."],"metadata":{"id":"mzfho5et-Idf"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/3RWxMwFy/Screenshot-2024-06-25-at-23-58-23.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=900 height=300></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"noA-cjy--oey","executionInfo":{"status":"error","timestamp":1720371493074,"user_tz":-240,"elapsed":4,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"7e12b7eb-ba0d-4ad8-9993-19165ddd2514"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'HTML' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2a38196b617f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://i.postimg.cc/3RWxMwFy/Screenshot-2024-06-25-at-23-58-23.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcentered_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'<div style=\"text-align:center;\"><img src=\"{url}\" width=900 height=300></div>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentered_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"]}]},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/d3jtJhDw/Screenshot-2024-06-26-at-00-18-57.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"XPpiBVM8D2eU","executionInfo":{"status":"ok","timestamp":1719346948594,"user_tz":-240,"elapsed":375,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"03b19180-f376-4dc5-d145-4bea1b65d055"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/d3jtJhDw/Screenshot-2024-06-26-at-00-18-57.png\" width=1000 height=500></div>"]},"metadata":{}}]},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/L6m8N30X/Screenshot-2024-06-26-at-00-18-45.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1200 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"Sm6cZzeJD-xS","executionInfo":{"status":"ok","timestamp":1719346971988,"user_tz":-240,"elapsed":367,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"212b4e7b-1e42-43ff-ade2-9f02f368e8db"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/L6m8N30X/Screenshot-2024-06-26-at-00-18-45.png\" width=1200 height=500></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# What kinds of things does pretraining teach?"],"metadata":{"id":"kuxCJ9J9D1U_"}},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/LXfrXjqH/Screenshot-2024-06-10-at-02-13-41.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"jvK-kDqhG9Hm","executionInfo":{"status":"ok","timestamp":1717971973759,"user_tz":-240,"elapsed":2,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"062e98eb-99fe-4da2-8438-f14fe47399da"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/LXfrXjqH/Screenshot-2024-06-10-at-02-13-41.png\" width=1000 height=500></div>"]},"metadata":{}}]},{"cell_type":"code","source":["# @title\n","url = \"https://i.postimg.cc/2jQsNFRG/Screenshot-2024-06-10-at-02-15-32.png\"\n","centered_image = HTML(f'<div style=\"text-align:center;\"><img src=\"{url}\" width=1000 height=500></div>')\n","display(centered_image)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"cellView":"form","id":"jTkHvNd4G_N-","executionInfo":{"status":"ok","timestamp":1717971985541,"user_tz":-240,"elapsed":4,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"cb3f4a3a-d845-41c8-bbbd-21c0ece9307b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><img src=\"https://i.postimg.cc/2jQsNFRG/Screenshot-2024-06-10-at-02-15-32.png\" width=1000 height=500></div>"]},"metadata":{}}]},{"cell_type":"code","source":["# @title\n","video_id = \"zjkBMFhNj_g\"  # Replace with the actual YouTube video ID\n","\n","embed_code = f'<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/{video_id}\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n","\n","centered_video = HTML(f'<div style=\"text-align:center;\">{embed_code}</div>')\n","display(centered_video)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"cellView":"form","id":"fH8iFGdnHCGX","executionInfo":{"status":"ok","timestamp":1717972269334,"user_tz":-240,"elapsed":2,"user":{"displayName":"Raphael Kalandadze","userId":"12455115696773085679"}},"outputId":"688fc801-0af3-4afb-d730-5c24d104bfe9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"text-align:center;\"><iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/zjkBMFhNj_g\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# asd\n","\n","- modern llm pretraining\n","- in context learning / chain of thought\n"],"metadata":{"id":"F2kDbV6aj3Lk"}},{"cell_type":"code","source":[],"metadata":{"id":"1HouE21lbz5O"},"execution_count":null,"outputs":[]}]}