{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Quiz: Transformers"}, {"cell_type": "markdown", "metadata": {}, "source": "### Multiple Choice Questions\n\n1. What is the core operation that enables Transformers to model dependencies between all positions in a sequence?  \na) Convolution  \nb) Recurrent connections  \nc) Attention mechanism  \nd) Pooling\n\n2. What is the purpose of positional encoding in Transformers?  \na) To normalize the input  \nb) To provide information about token positions in a sequence  \nc) To sort words alphabetically  \nd) To increase the model size\n\n3. In the self-attention mechanism, what are the Query, Key, and Value used for?  \na) They are fixed word embeddings  \nb) They represent the model\u2019s vocabulary  \nc) They are projections used to compute attention weights  \nd) They are used only during training\n\n4. Which Transformer component is **responsible for learning context-dependent representations of tokens**?  \na) Embedding layer  \nb) Positional encoder  \nc) Feedforward network  \nd) Multi-head self-attention\n\n5. Which of the following is **not** a typical feature of Transformer architecture?  \na) Layer normalization  \nb) Residual connections  \nc) Gated recurrent units  \nd) Feedforward layers\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Analytical Questions\n\n1. Why is attention considered more efficient and scalable than recurrence for sequence modeling?\n\n2. Describe how multi-head attention improves upon single-head attention in a Transformer.\n\n3. Explain how a Transformer handles very long sequences. What problems can arise, and how are they addressed?\n\n4. You want to fine-tune a pre-trained Transformer on a domain-specific dataset. What considerations should you keep in mind?\n\n5. Compare the roles of the encoder and decoder in a Transformer architecture for sequence-to-sequence tasks (e.g., translation).\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}