{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "cells": [{"id": "7533cdad", "cell_type": "markdown", "source": "# Quiz: Fine-Tuning Hyperparameters for Optimal Performance", "metadata": {}}, {"id": "58cc9114", "cell_type": "markdown", "source": "### Multiple Choice Questions\n\n1. What is the primary goal of fine-tuning hyperparameters in a model?  \na) To reduce model size  \nb) To increase training time  \nc) To optimize performance on the validation/test set  \nd) To avoid using regularization\n\n2. Which of the following is typically considered a hyperparameter?  \na) Weight in a neural network  \nb) Bias in a neuron  \nc) Learning rate  \nd) Output of a neuron\n\n3. What can happen if the learning rate is too high?  \na) Faster convergence with better accuracy  \nb) Model converges too slowly  \nc) Model may overshoot and fail to converge  \nd) Underfitting due to low weights\n\n4. Which strategy helps prevent overfitting during training?  \na) Increasing batch size  \nb) Early stopping  \nc) Using fewer epochs  \nd) Shuffling the validation set\n\n5. What does a small batch size generally lead to?  \na) Faster GPU performance  \nb) Smoother loss curves  \nc) No effect on generalization  \nd) Noisy gradient estimates and regularization\n\n6. Which of these is not a typical validation strategy?  \na) K-fold cross validation  \nb) Leave-one-out validation  \nc) Training loss monitoring  \nd) Hold-out set\n\n7. Which hyperparameter is usually tuned in conjunction with learning rate?  \na) Loss function  \nb) Optimizer  \nc) Number of epochs  \nd) Activation function\n\n8. What does dropout help with?  \na) Faster convergence  \nb) Overfitting  \nc) Underfitting  \nd) Batch normalization\n\n9. Which optimizer adapts learning rate based on gradient history?  \na) SGD  \nb) Adam  \nc) RMSprop  \nd) Both b and c\n\n10. If validation loss starts increasing while training loss keeps decreasing, what does it indicate?  \na) Underfitting  \nb) Learning rate too low  \nc) Overfitting  \nd) Batch size too small\n", "metadata": {}}, {"id": "0393400b", "cell_type": "markdown", "source": "### Analytical Questions\n\n1. How would you choose an appropriate batch size for training a neural network with limited memory?\n\n2. Describe a method to systematically tune multiple hyperparameters at once (e.g., learning rate, dropout, optimizer).\n\n3. Why is it important to monitor validation loss in addition to training loss?\n\n4. Suppose your model performs well on training but poorly on validation. What hyperparameters would you investigate and why?\n\n5. What is the trade-off between training for more epochs and overfitting? How can early stopping help?\n\n6. Describe how grid search and random search differ for hyperparameter tuning. When would you prefer one over the other?\n\n7. You increased the learning rate and observed unstable training. What would be your next steps?\n\n8. How can tuning the weight decay (L2 regularization) influence your model\u2019s performance?\n\n9. Why is it important to fix a random seed during hyperparameter search?\n\n10. Describe a real-world scenario where improper hyperparameter tuning might lead to incorrect business decisions.\n", "metadata": {}}]}