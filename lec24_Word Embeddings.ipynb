{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### გამოყენებული ლიტერატურა:\n",
    "- https://lena-voita.github.io/nlp_course/word_embeddings.html"
   ],
   "id": "ae032b23344fa548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings\n",
    "\n",
    "მანქანური სწავლების მოდელები მონაცემებს ჩვენგან (ადამიანებისგან) განსხვავებულად აღიქვამენ. მაგალითად, ჩვენ ადვილად ვიგებთ ტექსტის მნიშვნელობას \"მე დავინახე კატა\", მაგრამ ჩვენი მოდელები ამას ვერ ახერხებენ - მათ სჭირდებათ სიტყვის რიცხვითი რეპრეზენტაცია. ასეთი რეპრეზენტაციას, რომლის აღქმაც მოდელს მარტივად შეუძლია სიტყვის embedding ვექტორი ეწოდება.\n",
    "\n",
    "ჩვენი მიზანია თითოეულ სიტყვას შევუსაბამოთ მრავალგანზომილებიანი ვექტორი, ან, სხვა სიტყვებით რომ ვთქვათ, თითოეული სიტყვა განვათავსოთ მრავალგანზომილებიან სივრცეში ისეთ წერტილში, რომ მსგავსი მნიშვნელობისა და ერთმანეთთან დაკავშირებული სიტყვები ამ სივრცეშიც გეომეტრიულად ახლოს იყონ, ხოლო საპირისპირო ან დაუკავშირებელი სიტყვები კი - შორს."
   ],
   "id": "6f571c4bfa548a57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### საძიებელი ცხრილი (ლექსიკონი)\n",
    "როგორც წესი, დიდი მოცულობის ტექსტების გაანალიზებით მოცემული გვაქვს ყველა შესაძლო სიტყვის ლექსიკონი (vocabulary). ლექსიკონის სიტყვები შეგვიძლია გადავნომროთ და შესაბამის ადგილას საძიებელ ცხრილში შევინახოთ მისი embedding ვექტორი.\n",
    "\n",
    "უცნობი სიტყვების გასათვალისწინებლად (ისინი, რომლებიც არ არის ლექსიკონში), ჩვეულებრივ ლექსიკონში შედის სპეციალური ნიშანი UNK. განსხვავებულ მიდგომაში შეიძლება უცხო სიტყვები დავაიგნოროთ ან მოვანიჭოთ ნულოვანი ვექტორი.\n",
    "\n",
    "ამ ლექციის მთავარი კითხვაა: როგორ ვიღებთ ამ სიტყვის embedding ვექტორებს?"
   ],
   "id": "23d0630e5d8983fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif\" width=600 style=\"display: block; margin: 0 auto\">",
   "id": "462a59fc7b05b185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### დისკრეტული რეპრეზენტაცია: One-hot ვექტორები\n",
    "\n",
    "ყველაზე მარტივი, რისი გაკეთებაც შეგვიძლია არის სიტყვების წარმოდგენა One-hot ვექტორების სახით: ლექსიკონში i-ური სიტყვისთვის, ვექტორს აქვს 1 i-ურ განზომილებაზე და დანარჩენ განზომილებებზე - 0. მანქანური სწავლებისას ეს არის categorical მახასიათებლების წარმოდგენის ყველაზე მარტივი გზა.\n",
    "\n",
    "ალბათ ხვდებით, რატომ არ არის ეს ვექტორები სიტყვების რეპრეზეტაციის საუკეთესო გზა. ერთ-ერთი პრობლემა ისაა, რომ დიდი ლექსიკონებისთვის ეს ვექტორები ძალიან გრძელი იქნება: მათი განზომილება ლექსიკონის ზომის ტოლია (როგორც წესი რამდენიმე ასეული ათასი). ეს არასასურველია, მაგრამ ეს არ არის ყველაზე მნიშვნელოვანი პრობლემა.\n",
    "\n",
    "გაცილებით უფრო მნიშვნელოვანია ის ფაქტი, რომ ამ ტიპის ვექტორებმა არაფერი იციან იმ სიტყვებზე რასაც ისინი წარმოადგენენ. მაგალითად one-hot ვექტორებისთვის \"კატა\" ისეთივე ახლოსაა \"ძაღლთან\" როგორც \"მაგიდასთან\"! შეგვიძლია ვთქვათ რომ one-hot ვერ აღწერენ სიტყვის მნიშვნელობას.\n",
    "\n",
    "მაგრამ როგორ უნდა აღვწეროთ სიტყვების მნიშვნელობა?"
   ],
   "id": "325d647d397f04ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://lena-voita.github.io/resources/lectures/word_emb/one_hot-min.png\" width=300 style=\"display: block; margin: 0 auto\">",
   "id": "9002ba7fe79476bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### სიტყვების დისტრიბუციული რეპრეზენტაცია \n",
    "\n",
    "იმისათვის რომ სიტყვების ვექტორებში მათი მნიშვნელობების მიხედვით აღვბეჭდოთ, ჯერ უნდა განვსაზღვროთ თუ რას ნიშნავს სიტყვის \"მნიშვნელობა\". ამისათვის შევეცადოთ გავიგოთ თუ როგორ ვიგებთ ჩვენ, ადამიანები, რომელ სიტყვებს აქვთ მსგავსი მნიშვნელობები.\n",
    "\n",
    "1) ____-ს ბოთლი მაგიდაზე დევს.\n",
    "2) ქართველებს გვიყვარს ____.\n",
    "3) ____ მათრობელაა.\n",
    "4) ____ ყურძნისგან კეთდება.\n",
    "\n",
    "მას შემდეგ რაც თქვენ დაინახეთ უცნობი სიტყვა (____) სხვადასხვა კონტექსტში, თქვენ შეძელით მისი მნიშვნელობის გაგება. როგორ მოახერხეთ ეს?\n",
    "\n",
    "სავარაუდოდ, თქვენი ტვინი ეძებდა სხვა სიტყვებს, რომლებიც შეიძლება გამოყენებულ იქნან იმავე კონტექსტში (მაგ., ღვინო) და გამოიტანა დასკვნა რომ გამოტოვებულ სიტყვას მსგავსი მნიშვნელობა უნდა ჰქონდეს. დისტრიბუციულობის ჰიპოთეზა:\n",
    "\n",
    "_სიტყვებს, რომლებსაც ხშირად ვხვდებით მსგავს კონტექსტებში, მსგავსი მნიშვნელობები აქვთ._\n",
    "\n",
    "ეს საკმაოდ მძლავრი იდეაა: მისი გამოყენება პრაქტიკაშიც შეგვიძლია, რათა სიტყვების ვექტორებმა მათი მნიშვნელობები ასახონ. დისტრიბუციული ჰიპოთეზის თანახმად \"მნიშვნელობის დაჭერა\" და \"კონტექსტების დაჭერა\" არსებითად ერთიდაიგივეა, ამიტომ  ჩვენ უნდა შევიტანოთ სიტყვის კონტექსტის შესახებ ინფორმაცია მის ვექტორულ რეპრეზენტაციაში."
   ],
   "id": "b03654c7bd86733e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Word2Vec\n",
    "\n",
    "მთავარი იდეა: როგორ მივიღოთ სიტყვების embedding-ები? დავატრენინგოთ სიტყვების ვექტორები ამ სიტყვის კონტექსტის გამოცნობაზე.\n",
    "\n",
    "Word2Vec არის მოდელი, რომლის პარამეტრებიც არის თავად სიტყვების ვექტორები. ეს პარამეტრები ოპტიმიზირდება იტერაციულად გარკვეული მიზნისთვის. მიზანი აიძულებს სიტყვების ვექტორებს „იცოდნენ“ ის კონტექსტები, რომლებშიც შეიძლება სიტყვა გამოჩნდეს: ეს ვექტორები ტრენინგდება შესაბამისი სიტყვების შესაძლო კონტექსტების პროგნოზირებისთვის. როგორც დისტრიბუციული ჰიპოთეზიდან გახსოვთ, თუ ვექტორებს „ესმით“ კონტექსტები მაშინ მათ „ესმით“ სიტყვის მნიშვნელობა.\n",
    "\n",
    "Word2Vec იტერაციული მეთოდია და მისი ალგორითმი ასეთია:\n",
    "\n",
    "- აიღეთ დიდი ტექსტური კორპუსი;\n",
    "- გადაუყევით ტექსტს სიტყვა-სიტყვა და ყოველ ნაბიჯზე მიმდინარე (ცენტრალური) სიტყვისათვის გამოთვალეთ კონტექსტში მყოფი სხვა სიტყვების ალბათობები. კონტექსტის ფანჯრის ზომა პარამეტრია;\n",
    "- შეცვალეთ ეს ვექტორები ამ ალბათობების გასაზრდელად."
   ],
   "id": "e864a0a4fa356b28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://lena-voita.github.io/resources/lectures/word_emb/w2v/window_prob4-min.png\" width=600 style=\"display: block; margin: 0 auto\">\n",
   "id": "e517c6bd09d6548e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Word2Vec ვარიანტები: Skip-gram და CBOW\n",
    "\n",
    "არსებობს Word2Vec-ის ორი ვარიანტი: Skip-Gram და CBOW.\n",
    "\n",
    "Skip-Gram არის მოდელი, რომელიც ზემოთ განვიხილეთ: ის პროგნოზირებს ცენტრალური სიტყვის მიხედვით ცენტრალური პროგნოზირებს იმავე კონტექსტში მყოფ სხვა სიტყვებს. Skip-Gram negative sampling-ით ყველაზე პოპულარული მიდგომაა.\n",
    "\n",
    "CBOW (Continuous Bag-of-Words) პროგნოზირებს ცენტრალურ სიტყვას კონტექსტური სიტყვების ვექტორების ჯამის მიხედვით.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "73c3e006557fa7f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://lena-voita.github.io/resources/lectures/word_emb/w2v/cbow_skip-min.png\" width=600 style=\"display: block; margin: 0 auto\">\n",
   "id": "74832d67fc1af690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89cc3b5fcbd80c34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
